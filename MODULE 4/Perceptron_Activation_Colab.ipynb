{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2442e29",
      "metadata": {
        "id": "c2442e29"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, make_moons, make_circles, make_blobs\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "np.random.seed(7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158eb772",
      "metadata": {
        "id": "158eb772"
      },
      "source": [
        "\n",
        "## 1) The Historical Perceptron (Rosenblatt, 1957)\n",
        "\n",
        "A single neuron that computes a weighted sum of inputs and applies a **step** activation:\n",
        "\n",
        "$$ y = \\begin{cases} 1, & \\mathbf{w}^\\top \\mathbf{x} + b \\ge 0 \\\\ 0, & \\text{otherwise} \\end{cases} $$\n",
        "\n",
        "**Learning rule:**\n",
        "- If prediction is wrong, nudge weights toward the correct class:\n",
        "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\,(y - \\hat{y})\\, \\mathbf{x} $$\n",
        "$$ b \\leftarrow b + \\eta \\, (y - \\hat{y}) $$\n",
        "\n",
        "Works for **linearly separable** data, fails for **XOR**-type patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b891ed4d",
      "metadata": {
        "id": "b891ed4d"
      },
      "outputs": [],
      "source": [
        "# Linearly separable toy dataset\n",
        "X, y = make_classification(n_samples=400, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1, class_sep=2.0, random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JvfSc4-GniKi",
      "metadata": {
        "id": "JvfSc4-GniKi"
      },
      "outputs": [],
      "source": [
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5-1crcvnpCS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5-1crcvnpCS",
        "outputId": "3f6eda25-7f72-4523-b706-eb8b0b1a51e6"
      },
      "outputs": [],
      "source": [
        "clf = Perceptron(max_iter=1000, eta0=0.1, random_state=7, tol=1e-5)\n",
        "clf.fit(Xtr, ytr)\n",
        "pred = clf.predict(Xte)\n",
        "acc = accuracy_score(yte, pred)\n",
        "print(f\"Perceptron accuracy on linearly separable data: {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gUtb-UMQnkkE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gUtb-UMQnkkE",
        "outputId": "996ff4c6-5045-4052-8248-1b4b2712ceed"
      },
      "outputs": [],
      "source": [
        "# Plot decision boundary (single plot only)\n",
        "h = 0.02\n",
        "x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "plt.figure()\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(Xtr[:,0], Xtr[:,1], c=ytr, alpha=0.8)\n",
        "plt.title(\"Perceptron on Linearly Separable Data\")\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbb027d",
      "metadata": {
        "id": "fbbb027d"
      },
      "outputs": [],
      "source": [
        "# Nonlinear pattern (moons) — perceptron will struggle\n",
        "Xm, ym = make_moons(n_samples=400, noise=0.2, random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0keMFff1nzuR",
      "metadata": {
        "id": "0keMFff1nzuR"
      },
      "outputs": [],
      "source": [
        "Xm_tr, Xm_te, ym_tr, ym_te = train_test_split(Xm, ym, test_size=0.3, random_state=7, stratify=ym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mp4ode-an2yH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp4ode-an2yH",
        "outputId": "8c2f5a57-e9de-4fa8-8e06-78a8fb364e6a"
      },
      "outputs": [],
      "source": [
        "p_moon = Perceptron(max_iter=2000, eta0=0.1, random_state=7, tol=1e-5)\n",
        "p_moon.fit(Xm_tr, ym_tr)\n",
        "predm = p_moon.predict(Xm_te)\n",
        "accm = accuracy_score(ym_te, predm)\n",
        "print(f\"Perceptron accuracy on nonlinearly separable data (moons): {accm:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l7zR1Lien5u0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "l7zR1Lien5u0",
        "outputId": "31cd5bcb-0f3f-4426-875e-57398a637b90"
      },
      "outputs": [],
      "source": [
        "# Plot decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = Xm[:,0].min()-1, Xm[:,0].max()+1\n",
        "y_min, y_max = Xm[:,1].min()-1, Xm[:,1].max()+1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = p_moon.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "plt.figure()\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(Xm_tr[:,0], Xm_tr[:,1], c=ym_tr, alpha=0.8)\n",
        "plt.title(\"Perceptron Fails on Nonlinear (Moons)\")\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ebba83",
      "metadata": {
        "id": "b2ebba83"
      },
      "source": [
        "\n",
        "## 2) Core Activation Functions\n",
        "\n",
        "**Sigmoid**: $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ — squashes to (0,1).  \n",
        "**Tanh**: $$ \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$ — squashes to (-1,1).  \n",
        "**ReLU**: $$ \\text{ReLU}(x) = \\max(0, x) $$ — piecewise linear, non-saturating for \\(x>0\\).  \n",
        "**Softmax (vector)**: $$ \\mathrm{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $$ — turns scores into a probability simplex for multi-class.\n",
        "\n",
        "Below we plot each activation and its derivative to see how gradients behave.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e13ce1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "1e13ce1a",
        "outputId": "0d36f4e0-7d9a-4086-e180-a49d4a318be6"
      },
      "outputs": [],
      "source": [
        "# Plot functions (one figure per chart, as requested)\n",
        "xs = np.linspace(-7, 7, 400)\n",
        "\n",
        "# Define activations and derivatives\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def dsigmoid(x):\n",
        "    s = sigmoid(x)\n",
        "    return s*(1-s)\n",
        "# # Sigmoid\n",
        "plt.figure()\n",
        "plt.plot(xs, sigmoid(xs))\n",
        "plt.title(\"Sigmoid\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"σ(x)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xs, dsigmoid(xs))\n",
        "plt.title(\"Sigmoid Derivative\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"σ'(x)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "agfqEql_oyDU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "agfqEql_oyDU",
        "outputId": "3ce6e2c1-0806-44ed-b1b7-c5c199dc18be"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def dtanh(x):\n",
        "    t = np.tanh(x)\n",
        "    return 1 - t**2\n",
        "\n",
        "# Tanh\n",
        "plt.figure()\n",
        "plt.plot(xs, tanh(xs))\n",
        "plt.title(\"Tanh\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"tanh(x)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xs, dtanh(xs))\n",
        "plt.title(\"Tanh Derivative\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"d/dx tanh(x)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "omC-t3b6o5zZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "omC-t3b6o5zZ",
        "outputId": "a1e3da40-9cb1-4f40-de52-95e883708406"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def drelu(x):\n",
        "    return (x>0).astype(float)\n",
        "\n",
        "# ReLU\n",
        "plt.figure()\n",
        "plt.plot(xs, relu(xs))\n",
        "plt.title(\"ReLU\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"ReLU(x)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xs, drelu(xs))\n",
        "plt.title(\"ReLU Derivative\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"d/dx ReLU(x)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49691bf",
      "metadata": {
        "id": "a49691bf"
      },
      "source": [
        "\n",
        "## 3) Vanishing Gradient (and why ReLU helps)\n",
        "\n",
        "Sigmoid and tanh **saturate** for large |x|: their derivatives shrink toward 0.  \n",
        "In deep nets, repeated multiplication of small derivatives makes gradients **vanish**, slowing or stalling learning.\n",
        "\n",
        "Below, we simulate gradient magnitudes across layers assuming pre-activations are roughly Gaussian.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48bbbb1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "48bbbb1b",
        "outputId": "4a1bb04c-c8dc-491d-ff14-8db445b6baf9"
      },
      "outputs": [],
      "source": [
        "# Simulate gradient shrinkage across depth for different activations\n",
        "rng = np.random.default_rng(7)\n",
        "\n",
        "\n",
        "def expected_grad_scale(activation, samples=20000):\n",
        "  # for each activation:\n",
        "    if activation == \"relu\":\n",
        "      z = np.random.normal(0, np.sqrt(16), samples)  # He initialization\n",
        "      g = drelu(z)\n",
        "    elif activation == \"sigmoid\":\n",
        "        z = np.random.normal(0, 10, samples)  # Xavier for tanh/sigmoid\n",
        "        g = dsigmoid(z)\n",
        "    elif activation == \"tanh\":\n",
        "        z = np.random.normal(0, 10, samples)  # Xavier for tanh/sigmoid\n",
        "        g = dtanh(z)\n",
        "    else:\n",
        "        raise ValueError(\"unknown activation\")\n",
        "    return np.mean(np.abs(g))\n",
        "\n",
        "depths = np.arange(1, 51)\n",
        "scales = {a: [] for a in [\"sigmoid\", \"tanh\", \"relu\"]}\n",
        "\n",
        "base = {a: expected_grad_scale(a) for a in scales}\n",
        "for d in depths:\n",
        "    for a in scales:\n",
        "        scales[a].append(base[a]**d)\n",
        "\n",
        "for a in scales:\n",
        "    plt.figure()\n",
        "    plt.plot(depths, scales[a])\n",
        "    plt.title(f\"Expected Gradient Scale vs Depth — {a}\")\n",
        "    plt.xlabel(\"Depth (layers)\"); plt.ylabel(\"Relative scale\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3e90264",
      "metadata": {
        "id": "e3e90264"
      },
      "source": [
        "\n",
        "## 4) How Activations Shape Decision Surfaces\n",
        "\n",
        "We'll train shallow MLPs with different activations on 2D datasets and visualize the learned decision regions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-eF2cABIqWbT",
      "metadata": {
        "id": "-eF2cABIqWbT"
      },
      "outputs": [],
      "source": [
        "def plot_decision_surface(model, X, y, title):\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:,0].min() - 1.0, X[:,0].max() + 1.0\n",
        "    y_min, y_max = X[:,1].min() - 1.0, X[:,1].max() + 1.0\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, alpha=0.8)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1Hj8dBlqa_C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d1Hj8dBlqa_C",
        "outputId": "fc6954e3-3410-48f6-b6b2-be648ece2e3f"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    \"Circles\": make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=2),\n",
        " }\n",
        "for name, (Xd, yd) in datasets.items():\n",
        "    n_classes = len(np.unique(yd))\n",
        "    for act in [\"relu\", \"tanh\", \"logistic\"]:\n",
        "        clf = MLPClassifier(hidden_layer_sizes=(16,16), activation=act,\n",
        "                            learning_rate_init=0.05, max_iter=1000, random_state=7)\n",
        "        clf.fit(Xd, yd)\n",
        "        acc = clf.score(Xd, yd)\n",
        "        plot_decision_surface(clf, Xd, yd, f\"{name} — MLP ({act}) — accuracy={acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cv2fyzBLqakV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cv2fyzBLqakV",
        "outputId": "2afef7d0-9dab-4029-bcf9-e312a8623a5b"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    \"Blobs (3-class)\": make_blobs(n_samples=600, centers=3, cluster_std=1.5, n_features=2, random_state=3)\n",
        "}\n",
        "# For 3-class blobs, adapt output layer automatically\n",
        "for name, (Xd, yd) in datasets.items():\n",
        "    n_classes = len(np.unique(yd))\n",
        "    for act in [\"relu\", \"tanh\", \"logistic\"]:\n",
        "        clf = MLPClassifier(hidden_layer_sizes=(16,16), activation=act,\n",
        "                            learning_rate_init=0.05, max_iter=1000, random_state=7)\n",
        "        clf.fit(Xd, yd)\n",
        "        acc = clf.score(Xd, yd)\n",
        "        plot_decision_surface(clf, Xd, yd, f\"{name} — MLP ({act}) — accuracy={acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IV66IKIC4ACY",
      "metadata": {
        "id": "IV66IKIC4ACY"
      },
      "source": [
        "Note: Though sigmoid is binary classifer, we are using it here because sigmoid is used here for the hidden layers of MLPClassifier.\n",
        "The output layer is automatically chosen based on the task and\n",
        "If the target has > 2 classes, MLPClassifier uses softmax activation and cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f72574a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7f72574a",
        "outputId": "9f7546fe-3d30-4157-f0c7-9481eb967efb"
      },
      "outputs": [],
      "source": [
        "datasets= {\"Moons\": make_moons(n_samples=500, noise=0.2, random_state=2)}\n",
        "\n",
        "for name, (Xd, yd) in datasets.items():\n",
        "    n_classes = len(np.unique(yd))\n",
        "    for act in [\"relu\", \"tanh\", \"logistic\"]:\n",
        "        clf = MLPClassifier(hidden_layer_sizes=(16,16), activation=act,\n",
        "                            learning_rate_init=0.05, max_iter=1000, random_state=7)\n",
        "        clf.fit(Xd, yd)\n",
        "        acc = clf.score(Xd, yd)\n",
        "        plot_decision_surface(clf, Xd, yd, f\"{name} — MLP ({act}) — accuracy={acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad3ff48",
      "metadata": {
        "id": "3ad3ff48"
      },
      "source": [
        "\n",
        "## 5) When to Use What (Quick Guide)\n",
        "\n",
        "- **ReLU** (default for hidden layers): simple, fast, mitigates vanishing gradient for \\(x>0\\). Try **LeakyReLU/ELU/GELU** if many dead units.\n",
        "- **Tanh**: zero-centered outputs; good for small networks or when inputs roughly in \\([-1,1]\\). May suffer vanishing gradients.\n",
        "- **Sigmoid**: best for **output** of binary classification (with BCE loss). Avoid in deep hidden layers.\n",
        "- **Softmax**: use at the **output** for multi-class classification with cross-entropy.\n",
        "- Always pair with good **initialization** (He for ReLU, Xavier/Glorot for tanh/sigmoid) and **normalization** (BatchNorm/LayerNorm) to stabilize training.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d309206",
   "metadata": {
    "id": "6d309206"
   },
   "source": [
    "# ML Libraries Overview\n",
    "In this notebook, we'll explore three popular machine learning libraries: **scikit-learn**, **TensorFlow**, and **PyTorch**.\n",
    "We'll discuss when to choose each, how to set up the environment, and run minimal working demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e8106",
   "metadata": {
    "id": "c18e8106"
   },
   "source": [
    "## 1. When to choose which library\n",
    "| Feature                  | Scikit-learn | TensorFlow | PyTorch |\n",
    "|--------------------------|--------------|------------|---------|\n",
    "| **Best for**             | Classical ML | Deep Learning | Deep Learning |\n",
    "| **Model complexity**     | Simple to moderate | Complex neural nets | Complex neural nets |\n",
    "| **Ease of use**          | Very easy | Medium | Easy-medium |\n",
    "| **Speed on GPU**         | Limited | Excellent | Excellent |\n",
    "| **Ecosystem**            | Data preprocessing, model selection | End-to-end DL, production ready | Research, experimentation |\n",
    "\n",
    "**Rule of thumb:**\n",
    "- If youâ€™re doing **logistic regression, random forests, SVMs** â†’ use **scikit-learn**.\n",
    "- If youâ€™re training **neural networks for production** â†’ use **TensorFlow**.\n",
    "- If youâ€™re doing **deep learning research / custom architectures** â†’ use **PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8eb9c",
   "metadata": {
    "id": "c5b8eb9c"
   },
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "0da51461",
   "metadata": {
    "id": "0da51461"
   },
   "source": [
    "!pip install scikit-learn tensorflow torch --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49f4ddf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49f4ddf0",
    "outputId": "f8907b73-159d-44a6-b6a4-d4dfbed2fc19"
   },
   "source": [
    "import sklearn, tensorflow as tf, torch\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "print('TensorFlow:', tf.__version__)\n",
    "print('PyTorch:', torch.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a326f6ed",
   "metadata": {
    "id": "a326f6ed"
   },
   "source": [
    "## 3. Demo: Scikit-learn â€“ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "id": "3acdf8f5",
   "metadata": {
    "id": "3acdf8f5"
   },
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "df = sns.load_dataset('iris')\n",
    "df.head()"
   ],
   "metadata": {
    "id": "1Za8lknzjkQx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "e7fe5dda-fbed-4210-987e-9331bbe7fc62"
   },
   "id": "1Za8lknzjkQx",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()\n",
    "# #model 1 (logistic regression)\n",
    "# X = df.drop('species', axis=1)\n",
    "# y = LabelEncoder().fit_transform(df[\"species\"])\n",
    "\n",
    "#model 2 (linear regression)\n",
    "X = df.drop(columns=[\"petal_length\", \"species\"])\n",
    "y = df[\"petal_length\"]\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzqxkS8hP3fd",
    "outputId": "e9ef92bc-c70c-4d17-edb2-16b49b284f10"
   },
   "id": "MzqxkS8hP3fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "X = df.drop('species', axis=1): this will form a numby array with first four columns\n",
    "\n",
    "y = LabelEncoder().fit_transform(df[\"species\"]): : this will form a numby array with last column\n",
    "\n",
    "X â†’ Features (numeric values describing each flower, shape (150, 4) â€” 150 samples, 4 features).\n",
    "\n",
    "y â†’ Target labels (0, 1, 2 for different flower species)."
   ],
   "metadata": {
    "id": "8RX-P-yQjiPI"
   },
   "id": "8RX-P-yQjiPI"
  },
  {
   "cell_type": "code",
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "id": "Rq4LArPpjmxW"
   },
   "id": "Rq4LArPpjmxW",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "X_train â†’ Training features.\n",
    "\n",
    "X_test â†’ Testing features.\n",
    "\n",
    "y_train â†’ Training labels.\n",
    "\n",
    "y_test â†’ Testing labels."
   ],
   "metadata": {
    "id": "RmqWx5vUj8Ju"
   },
   "id": "RmqWx5vUj8Ju"
  },
  {
   "cell_type": "code",
   "source": [
    "# # Model 1 (logistic regression)\n",
    "# model = LogisticRegression(max_iter=200)\n",
    "# model.fit(X_train, y_train)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "UpQuJI_Rjod7",
    "outputId": "05697029-cfe1-4918-b38c-be76a5790fce"
   },
   "id": "UpQuJI_Rjod7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "max_iter=200 â†’ Sets the maximum number of optimization iterations (default can be too small for convergence, so we increase it).\n",
    "\n",
    ".fit() â†’ Trains the logistic regression model by finding the best weights for classification.\n",
    "\n",
    "The model learns patterns in X_train that correspond to the correct labels y_train."
   ],
   "metadata": {
    "id": "YMd6Pqofj48s"
   },
   "id": "YMd6Pqofj48s"
  },
  {
   "cell_type": "code",
   "source": [
    "# # Predict & evaluate (logistic regression)\n",
    "# y_pred = model.predict(X_test)\n",
    "# print('Accuracy:', accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4z1vfYRtjrAf",
    "outputId": "dd073041-3793-4a4b-b809-f8d0e41aa43b"
   },
   "id": "4z1vfYRtjrAf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "FYPBfDYgUZ7C",
    "outputId": "e7cd03ac-87b1-4eec-e51d-02ffac1f0a99"
   },
   "id": "FYPBfDYgUZ7C",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    ".predict(X_test) â†’ Uses the trained model to predict labels for unseen test data.\n",
    "\n",
    "Produces y_pred â€” an array of predicted class labels.\n",
    "\n",
    "accuracy_score(y_test, y_pred) â†’ Calculates the fraction of correct predictions (values between 0 and 1)."
   ],
   "metadata": {
    "id": "_AV337aUkI9l"
   },
   "id": "_AV337aUkI9l"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean Squared Error (MSE) is the most common loss function used in linear regression. It measures how far the predicted values are from the true values by squaring the difference and then averaging over all data points.\n",
    "\n",
    "Formula:\n",
    "MSE = (1/n) * Î£ (y_true - y_pred)Â²\n",
    "\n",
    "Where\n",
    "\n",
    "n = number of samples\n",
    "\n",
    "y_true = actual target values\n",
    "\n",
    "y_pred = predicted target values\n",
    "\n",
    "Why squared?\n",
    "\n",
    "Squaring ensures all errors are positive.\n",
    "\n",
    "It penalizes larger errors more strongly than smaller ones."
   ],
   "metadata": {
    "id": "kkIDoFC3smVp"
   },
   "id": "kkIDoFC3smVp"
  },
  {
   "cell_type": "code",
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXa7ZFcOWP-m",
    "outputId": "42dad193-b3ee-4aeb-c518-7ace03805c26"
   },
   "id": "AXa7ZFcOWP-m",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch vs TensorFlow: Linear Regression with Gradients\n",
    "\n",
    "We use a simple **linear regression** model:\n",
    "\\[\n",
    "$$ y \\approx wX + b $$\n",
    "\\]\n",
    "---\n",
    "\n",
    "## ðŸ”¹ What are Gradients?\n",
    "- The **loss** is the Mean Squared Error (MSE):\n",
    "\n",
    "\\[\n",
    "$$ L = \\frac{1}{N} \\sum (y - (wX+b))^2 $$\n",
    "\\]\n",
    "\n",
    "- We want **derivatives**:\n",
    "  - Gradient wrt `w`:  \\($\\frac{\\partial L}{\\partial w}$\\)  \n",
    "  - Gradient wrt `b`:  \\($\\frac{\\partial L}{\\partial b}$\\)\n",
    "\n",
    "These gradients tell us how much the loss changes when we tweak `w` or `b`.  \n",
    "They are essential for optimization (gradient descent).\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "HPwZM0Qzu2Cn"
   },
   "id": "HPwZM0Qzu2Cn"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Random data\n",
    "X = torch.rand(100, 1)\n",
    "y = 3*X + 2 + 0.1*torch.randn(100, 1)\n",
    "\n",
    "# Parameters (with gradients enabled)\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Prediction\n",
    "y_pred = w*X + b\n",
    "\n",
    "# Mean Squared Error\n",
    "loss = torch.mean((y - y_pred)**2)\n",
    "\n",
    "# Backprop (compute gradients)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Gradient w:\", w.grad.item(), \"Gradient b:\", b.grad.item())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYe-kEOjtSJd",
    "outputId": "c8141876-4b52-4c11-e20b-7a969643fa72"
   },
   "id": "BYe-kEOjtSJd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Random data\n",
    "X = tf.random.uniform((100, 1))\n",
    "y = 3*X + 2 + 0.1*tf.random.normal((100, 1))\n",
    "\n",
    "# Parameters as Variables\n",
    "w = tf.Variable(tf.random.normal((1,)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "# GradientTape to record operations\n",
    "with tf.GradientTape() as tape:\n",
    "    y_pred = w*X + b\n",
    "    loss = tf.reduce_mean((y - y_pred)**2)\n",
    "\n",
    "# Compute gradients wrt w and b\n",
    "grads = tape.gradient(loss, [w, b])\n",
    "print(\"Loss:\", loss.numpy())\n",
    "print(\"Gradient w:\", grads[0].numpy(), \"Gradient b:\", grads[1].numpy())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqZQRiwewDm-",
    "outputId": "bc07171e-1c63-4242-ea9c-0759a5872c6e"
   },
   "id": "yqZQRiwewDm-",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ec60439",
   "metadata": {
    "id": "0ec60439"
   },
   "source": [
    "## 4. Demo: TensorFlow â€“ Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "8c1400af",
   "metadata": {
    "id": "8c1400af"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Random data: 100 samples, 10 features\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.randint(0, 2, size=(100,))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "np.random.rand(100, 10) â†’ Creates a NumPy array of shape (100, 10) with random values between 0 and 1.\n",
    "\n",
    "Simulates 100 samples, each with 10 features.\n",
    "\n",
    "np.random.randint(0, 2, size=(100,)) â†’ Creates an array of random integers 0 or 1, shape (100,).\n",
    "\n",
    "This simulates binary classification labels (0 = class A, 1 = class B)."
   ],
   "metadata": {
    "id": "0IlJDRrti_w5"
   },
   "id": "0IlJDRrti_w5"
  },
  {
   "cell_type": "code",
   "source": [
    "# Build model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ],
   "metadata": {
    "id": "wTCW127-h8_1"
   },
   "id": "wTCW127-h8_1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "models.Sequential([...]) â†’ Creates a Sequential model, meaning layers are stacked one after the other.\n",
    "\n",
    "layers.Dense(16, activation='relu', input_shape=(10,)) â†’\n",
    "\n",
    "Fully connected layer with 16 neurons.\n",
    "\n",
    "activation='relu' â†’ Rectified Linear Unit activation, sets negative outputs to 0.\n",
    "\n",
    "input_shape=(10,) â†’ The model expects 10 features for each sample.\n",
    "\n",
    "layers.Dense(1, activation='sigmoid') â†’ Output layer:\n",
    "\n",
    "1 neuron â†’ outputs a single probability.\n",
    "\n",
    "Sigmoid activation squashes output between 0 and 1, suitable for binary classification."
   ],
   "metadata": {
    "id": "iiy5UuAOh6w4"
   },
   "id": "iiy5UuAOh6w4"
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "id": "2UZ9j2XOh_YL"
   },
   "id": "2UZ9j2XOh_YL",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "optimizer='adam' â†’ Optimization algorithm that adjusts weights using gradients (fast & adaptive learning rates).\n",
    "\n",
    "loss='binary_crossentropy' â†’ Loss function for binary classification using probabilities.\n",
    "\n",
    "metrics=['accuracy'] â†’ We want to track accuracy during training."
   ],
   "metadata": {
    "id": "hBIZKGspiCRI"
   },
   "id": "hBIZKGspiCRI"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Forward Pass**\n",
    "\n",
    "For each batch of data, TensorFlow runs the modelâ€™s call()/forward() function to compute predictions (y_pred).\n",
    "\n",
    "It applies the layers in sequence (Dense, ReLU, etc.).\n",
    "\n",
    "**Loss Computation**\n",
    "\n",
    "Compares predictions (y_pred) against ground truth (y) using the loss function defined in model.compile(...).\n",
    "\n",
    "**Gradient Calculation (Backpropagation)**\n",
    "\n",
    "Uses autograd (via tf.GradientTape) to automatically compute gradients of the loss w.r.t model parameters.\n",
    "\n",
    "**Optimizer Step**\n",
    "\n",
    "Updates weights using the optimizer (e.g., SGD, Adam) chosen in compile.\n",
    "\n",
    "\n",
    "**Metrics Tracking**\n",
    "\n",
    "Evaluates accuracy, loss, or other metrics on each batch/epoch if specified.\n",
    "\n",
    "**Looping**\n",
    "\n",
    "Repeats this process for each batch in each epoch."
   ],
   "metadata": {
    "id": "mubrhdNO2eHa"
   },
   "id": "mubrhdNO2eHa"
  },
  {
   "cell_type": "code",
   "source": [
    "# Train\n",
    "model.fit(X, y, epochs=5, batch_size=8)"
   ],
   "metadata": {
    "id": "sw9iv3YliApD"
   },
   "id": "sw9iv3YliApD",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "epochs=5 â†’ Go through the entire dataset 5 times.\n",
    "\n",
    "batch_size=8 â†’ Process data in batches of 8 samples before updating weights."
   ],
   "metadata": {
    "id": "_nKuXjXWiCrZ"
   },
   "id": "_nKuXjXWiCrZ"
  },
  {
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(X, y, verbose=0)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "id": "Xe6f3KQlfrF3"
   },
   "id": "Xe6f3KQlfrF3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X)\n",
    "\n",
    "# Convert probabilities to class labels (if classification)\n",
    "import numpy as np\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "print(\"Predicted classes:\", y_pred[:10])"
   ],
   "metadata": {
    "id": "AXmZqKgYgS4E"
   },
   "id": "AXmZqKgYgS4E",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6uOF2EhIftt5"
   },
   "id": "6uOF2EhIftt5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf684484",
   "metadata": {
    "id": "bf684484"
   },
   "source": [
    "## 5. Demo: PyTorch â€“ Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch â€“ the main PyTorch package for tensors and core operations.\n",
    "\n",
    "torch.nn â€“ contains tools to build neural networks (layers, activations, loss functions).\n",
    "\n",
    "torch.optim â€“ contains optimization algorithms like Adam, SGD, RMSP"
   ],
   "metadata": {
    "id": "yxJe0ipAgAP1"
   },
   "id": "yxJe0ipAgAP1"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "qNd9CVjKgDs4"
   },
   "id": "qNd9CVjKgDs4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Random data\n",
    "X = torch.rand(100, 10)\n",
    "y = torch.randint(0, 2, (100,), dtype=torch.float32)"
   ],
   "metadata": {
    "id": "9c83RbRWgF-g"
   },
   "id": "9c83RbRWgF-g",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6efba878",
   "metadata": {
    "id": "6efba878"
   },
   "source": [
    "# Model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.fc2(self.relu(self.fc1(x))))\n",
    "# Defines how data flows through the network:\n",
    "# Input x goes to fc1 layer.\n",
    "# Apply ReLU activation.\n",
    "# Output goes to fc2 layer.\n",
    "# Apply Sigmoid to get probability between 0 and 1."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "id": "BNklZyrqgjSv"
   },
   "id": "BNklZyrqgjSv",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Train loop\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X).squeeze()\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ],
   "metadata": {
    "id": "Evhbm500gk5X"
   },
   "id": "Evhbm500gk5X",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11f0ded7",
   "metadata": {
    "id": "11f0ded7"
   },
   "source": [
    "## 6. Summary\n",
    "- **scikit-learn** â†’ Best for small to medium datasets, quick models, classical algorithms.\n",
    "- **TensorFlow** â†’ Best for scalable deep learning models, production deployment.\n",
    "- **PyTorch** â†’ Best for flexible, experimental deep learning research."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

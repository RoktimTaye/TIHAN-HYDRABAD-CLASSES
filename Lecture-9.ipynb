{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "VL9eKbXDcx-5"
   },
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install category_encoders\n",
    "\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6n-30SFhw1p",
    "outputId": "ce7bb18b-73f0-4c19-ffe6-6f954a6dfd9e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import (MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler, QuantileTransformer,\n",
    "    LabelEncoder, OneHotEncoder, OrdinalEncoder)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "id": "cB9njTo7fcNi"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Load Dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select features & target\n",
    "df = titanic[['class', 'sex', 'age', 'fare', 'embarked', 'survived']].copy()\n",
    "\n",
    "# Introduce some missing values for demo (already present in Titanic)\n",
    "df.head()\n"
   ],
   "metadata": {
    "id": "-RvgG_GTfd9Q",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "00021002-5e34-4436-a81f-a291d88692f7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = df[[\"age\", \"fare\", \"sex\", \"class\", \"embarked\",  \"survived\"]].dropna()\n"
   ],
   "metadata": {
    "id": "eu1K5D7udYUf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Basic Data Info\n",
    "print(df.info())"
   ],
   "metadata": {
    "id": "7AtiIbShg-IT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e90bab0-86f9-4f10-ff00-b56f53d9f991"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(df.describe(include=\"all\").T)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GY03gDmmoX4S",
    "outputId": "1da65f32-b42e-4106-d7c7-07f4b6f1a413"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "info() shows datatypes (which helps us know which features are categorical/numeric).\n",
    "\n",
    "describe() summarizes statistics."
   ],
   "metadata": {
    "id": "Fy1ENqfxhQfs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "df.dropna(inplace=True)  # drop rows with missing values\n",
    "df.head()\n"
   ],
   "metadata": {
    "id": "5GUhVSXXhCtG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "8c02578f-ba95-4189-bdcc-5ab590c9fb12"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Scaling (Normalization & Standardization)**"
   ],
   "metadata": {
    "id": "IfHJ5xpCjofE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Feature Scaling: Normalization vs. Standardization\n",
    "\n",
    "Feature scaling is an essential preprocessing step in machine learning.  \n",
    "Many algorithms (Linear Regression, Logistic Regression, SVMs, PCA, K-means, Neural Networks) rely on **distances, dot-products, or gradient descent**. If features are on very different scales, the model may become biased toward larger-magnitude features.\n",
    "\n",
    "---\n",
    "\n",
    "## Normalization (Min–Max Scaling)\n",
    "\n",
    "**Definition:**  \n",
    "Rescales values of a feature into a fixed range, usually [0, 1].  \n",
    "\n",
    "\\[$\n",
    "x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$\n",
    "\\]\n",
    "\n",
    "- Each feature’s smallest value becomes 0, largest becomes 1.  \n",
    "- Preserves the shape of the distribution, but **compresses** the scale.  \n",
    "- Very sensitive to **outliers** (a single extreme value can stretch the scale).  \n",
    "\n",
    "**When to use:**  \n",
    "- Works well when features are bounded (e.g., pixel intensities between 0–255).  \n",
    "- Useful in **Neural Networks** where inputs are often normalized.  \n",
    "- Less suitable for features with heavy outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## Standardization (Z-score Scaling)\n",
    "\n",
    "**Definition:**  \n",
    "Centers the feature at mean 0 and scales it to unit variance.  \n",
    "\n",
    "\\[$\n",
    "x' = \\frac{x - \\mu}{\\sigma}$\n",
    "\\]\n",
    "\n",
    "where \\(\\mu\\) is the feature mean and \\(\\sigma\\) is the standard deviation.  \n",
    "\n",
    "- The transformed feature has mean ≈ 0 and standard deviation ≈ 1.  \n",
    "- Not limited to [0, 1]; values can be negative or greater than 1.  \n",
    "- More **robust** than normalization when dealing with outliers.  \n",
    "- Assumes features are roughly Gaussian (bell-shaped) for best results.  \n",
    "\n",
    "**When to use:**  \n",
    "- Default choice for **Linear/Logistic Regression, SVMs, PCA, and K-means**.  \n",
    "- Works well when features have different units (e.g., age in years, fare in dollars).  \n",
    "- Preserves outlier influence without compressing them to 0–1.\n",
    "\n",
    "---\n",
    "\n",
    "##  Choosing Between Normalization and Standardization\n",
    "\n",
    "- If your model relies on **distance or dot-product geometry** (linear regression, SVM, PCA, neural networks) → **Standardization** is often better.  \n",
    "- If you need features in a **bounded range** (e.g., image intensities, probability inputs) → **Normalization** works well.  \n",
    "- With **tree-based models** (Decision Trees, Random Forests, XGBoost, CatBoost), scaling usually **doesn’t matter** because splits are not distance-based.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Titanic Example (numeric features only)\n",
    "\n",
    "In the Titanic dataset, consider two features:  \n",
    "- **`age`** (in years, typically 0–80 but with some missing values).  \n",
    "- **`fare`** (in dollars, highly skewed with very large outliers).  \n",
    "\n",
    "- With **Normalization**, `fare` outliers (very expensive tickets) will squeeze the majority of passenger fares into a very narrow [0–0.1] range.  \n",
    "- With **Standardization**, `fare` will be centered and scaled, but outliers will appear as large positive z-scores, which keeps more useful variation for linear models.  \n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "- **Normalization:** good when bounded [0,1] values are needed, but sensitive to outliers.  \n",
    "- **Standardization:** good for most ML algorithms, especially linear ones; robust to varying feature units.  \n"
   ],
   "metadata": {
    "id": "8tRTU-tZPCcR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_minmax = df.copy()\n",
    "df_minmax[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
    "\n",
    "print(df_minmax.head())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
    "df_minmax[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"MinMax Normalized Fare\", color=\"orange\")\n",
    "plt.show()\n",
    "# Rescales values between 0 and 1. Useful when features have different ranges.\n"
   ],
   "metadata": {
    "id": "r0La29mxobDI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "outputId": "4d5fdafc-148b-4238-d9ff-b451db2e8cbf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "df_standard = df.copy()\n",
    "df_standard[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
    "\n",
    "print(df_standard.head())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
    "df_standard[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"Standardized Fare\", color=\"green\")\n",
    "plt.show()\n",
    "# Transforms to mean=0, std=1. Best for models assuming Gaussian-like features."
   ],
   "metadata": {
    "id": "aR4sqJ8dotZ3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "outputId": "88801362-f779-423f-d0a5-11bfe1aee06c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  QuantileTransformer\n",
    "\n",
    "**What it does:**  \n",
    "- Transforms features to follow a **target distribution** by mapping quantiles.  \n",
    "- Two main options in scikit-learn:  \n",
    "  - `output_distribution=\"uniform\"` → transforms features into [0, 1] uniform distribution.  \n",
    "  - `output_distribution=\"normal\"` → transforms features into standard normal (mean=0, var=1).  \n",
    "\n",
    "**How it works:**  \n",
    "- Computes the empirical CDF (cumulative distribution function) of the data.  \n",
    "- Maps each value to its quantile, then to the chosen output distribution.  \n",
    "\n",
    "**Properties:**  \n",
    "- Makes distributions **more Gaussian-like** (if `normal`).  \n",
    "- Very robust to **outliers** (they get compressed into the tails).  \n",
    "- Non-linear transformation → changes relationships between features.  \n",
    "\n",
    "**When to use:**  \n",
    "- Features are highly skewed, long-tailed, or non-Gaussian.  \n",
    "- Useful before algorithms that are sensitive to non-normality (e.g., linear models, Gaussian-based methods).  \n",
    "- Works well when feature scales are very irregular.  \n",
    "\n",
    "---\n",
    "\n",
    "##  StandardScaler vs. QuantileTransformer\n",
    "\n",
    "| Aspect | StandardScaler | QuantileTransformer |\n",
    "|--------|----------------|----------------------|\n",
    "| Effect on Mean/Variance | Centers to mean 0, var 1 | Shapes data into uniform [0,1] or normal |\n",
    "| Handles Outliers | No (keeps them extreme) | Yes (compresses them into tails) |\n",
    "| Distribution Shape | Preserved (still skewed if original is skewed) | Changed (forces uniform or normal) |\n",
    "| Use Case | Roughly Gaussian data, not too skewed | Highly skewed, heavy-tailed, or irregular data |\n",
    "\n",
    "---\n",
    "\n",
    "##  Titanic Example (numeric feature: `fare`)\n",
    "\n",
    "- **StandardScaler:**  \n",
    "  - `fare` will be centered at 0, variance = 1.  \n",
    "  - Extremely high fares remain large positive z-scores.  \n",
    "- **QuantileTransformer (normal):**  \n",
    "  - `fare` distribution becomes closer to Gaussian.  \n",
    "  - Very expensive tickets are pushed into the far right tail but less extreme.  \n",
    "\n",
    "---\n",
    "\n",
    " **Summary:**  \n",
    "- Use **StandardScaler** when features are moderately well-behaved (close to Gaussian).  \n",
    "- Use **QuantileTransformer** when features are **skewed** or have **outliers**, and you want a Gaussian or uniform output distribution.  "
   ],
   "metadata": {
    "id": "Sg8oXdr8Q6yo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = QuantileTransformer(output_distribution=\"uniform\")\n",
    "df_quantile_uniform = df.copy()\n",
    "df_quantile_uniform[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
    "\n",
    "print(df_quantile_uniform.head())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
    "df_quantile_uniform[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"Quantile Uniform Fare\", color=\"brown\")\n",
    "plt.show()\n",
    "# Maps values → uniform [0,1] distribution."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "pHQ2wqaOcR4h",
    "outputId": "bf77b401-0aea-4417-bf61-83cf331dac4c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = QuantileTransformer(output_distribution=\"normal\")\n",
    "df_quantile_normal = df.copy()\n",
    "df_quantile_normal[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
    "\n",
    "print(df_quantile_normal.head())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
    "df_quantile_normal[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"Quantile Normal Fare\", color=\"cyan\")\n",
    "plt.show()\n",
    "# Forces values to follow a normal distribution."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "IHQ4AFmxcX_n",
    "outputId": "7e28506e-6a90-4555-f23e-779828c4765d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Encoding**"
   ],
   "metadata": {
    "id": "v-ICg81EjeRv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Why Do We Need Encoding?\n",
    "\n",
    "Machine learning models usually work with **numerical features**.  \n",
    "But in many datasets (like Titanic), we have **categorical features** such as:\n",
    "\n",
    "- `sex` → {male, female}  \n",
    "- `embarked` → {C, Q, S}  \n",
    "- `class` → {First, Second, Third}  \n",
    "\n",
    "These values are **labels**, not numbers.  \n",
    "If we feed them directly into most ML algorithms, they will not understand the categories.  \n",
    "\n",
    " **Encoding** is the process of converting categorical variables into numeric form so that models can use them.\n",
    "\n",
    "---\n",
    "\n",
    "##  Label Encoding\n",
    "\n",
    "**How it works:**  \n",
    "- Assigns each category a unique integer.  \n",
    "\n",
    "Example:  \n",
    "- `sex`: {male → 0, female → 1}  \n",
    "- `embarked`: {C → 0, Q → 1, S → 2}  \n",
    "\n",
    "**Advantages:**  \n",
    "- Simple and compact.  \n",
    "- Works well for **tree-based models** (Decision Trees, Random Forest, XGBoost), since they split based on thresholds, not distances.  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- Imposes a **fake order** (0 < 1 < 2), which can mislead models like Linear Regression, Logistic Regression, or SVM (they may think `S` > `Q` > `C`).  \n",
    "\n",
    "---\n",
    "\n",
    "##  One-Hot Encoding (OHE)\n",
    "\n",
    "**How it works:**  \n",
    "- Creates a new **binary column for each category**.  \n",
    "- Value is `1` if the row belongs to that category, else `0`.  \n",
    "\n",
    "Example: `embarked` → {C, Q, S}  \n",
    "\n",
    "| embarked | embarked_C | embarked_Q | embarked_S |\n",
    "|----------|------------|------------|------------|\n",
    "| C        | 1          | 0          | 0          |\n",
    "| S        | 0          | 0          | 1          |\n",
    "| Q        | 0          | 1          | 0          |\n",
    "\n",
    "**Advantages:**  \n",
    "- Avoids false ordinal relationships.  \n",
    "- Safe default for **linear models, logistic regression, SVMs, and neural networks**.  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- Increases the number of features (can be large if categories are many).  \n",
    "- Can lead to **sparse matrices** with high-cardinality categorical variables.  \n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Which?\n",
    "\n",
    "- **Tree-based models (Decision Trees, Random Forest, Gradient Boosting):**  \n",
    "  → Label Encoding is fine (trees handle categories by splitting, not by distances).  \n",
    "\n",
    "- **Linear models, Logistic Regression, SVMs, Neural Networks:**  \n",
    "  → One-Hot Encoding is preferred (no fake ordering).  \n",
    "\n",
    "- **High cardinality (hundreds/thousands of categories):**  \n",
    "  → Consider advanced techniques like **Target Encoding, Frequency Encoding, or Hashing**.  \n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "- **Label Encoding** → compact, but risky for linear models.  \n",
    "- **One-Hot Encoding** → more features, but safer for most models.  \n",
    "- Always choose encoding based on **model type** and **data characteristics**.  \n"
   ],
   "metadata": {
    "id": "qAdgBW37SE1J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "categorical_features = [\"sex\", \"embarked\",\"class\"]\n",
    "\n",
    "# 1. Label Encoding\n",
    "le = LabelEncoder()\n",
    "df_label = df.copy()\n",
    "for col in categorical_features:\n",
    "    df_label[col] = le.fit_transform(df[col])\n",
    "print(\"Label Encoding:\\n\", df_label, \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qd5IpA0Nc39Y",
    "outputId": "add16f7a-bd09-4b56-974c-bc3580ec7c6f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "print(\"One-Hot Encoding:\\n\", df_onehot.head(), \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMumJlIGc_nb",
    "outputId": "11749cc8-ba9b-44e5-e3df-90fed95dc61f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🎛️ Beyond One-Hot and Label Encoding: Advanced Encodings\n",
    "\n",
    "\n",
    "##  Ordinal Encoding\n",
    "\n",
    "**What it does:**  \n",
    "- Maps categories to integers **preserving their natural order**.  \n",
    "\n",
    "Example (Titanic `class`):  \n",
    "- First → 3  \n",
    "- Second → 2  \n",
    "- Third → 1  \n",
    "\n",
    "**Why:**  \n",
    "- `class` is truly ordered (First > Second > Third).  \n",
    "- Unlike generic Label Encoding, the ordering here makes sense.  \n",
    "\n",
    "**Use case:**  \n",
    "- Works well for features with meaningful hierarchy (education level, size: small/medium/large, etc.).  \n",
    "- Safe for linear models if the relationship is monotonic.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "Dflq0xVSSuXI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Ordinal Encoding (for ordered categories like 'class')\n",
    "ord_enc = OrdinalEncoder(categories=[[\"Third\", \"Second\", \"First\"]])\n",
    "df_ord = df.copy()\n",
    "df_ord[\"class\"] = ord_enc.fit_transform(df[[\"class\"]])\n",
    "print(\"Ordinal Encoding:\\n\", df_ord[[\"class\"]], \"\\n\")\n",
    "# Ordinal Encoding (for ordered categories like class = [Third < Second < First])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V629_goxeyRK",
    "outputId": "75bbf38f-f3bc-4d75-f78c-85a5c13b8d8b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ Target Encoding\n",
    "\n",
    "**What it does:**  \n",
    "- Replaces each category with a **statistic of the target variable** (commonly the mean).  \n",
    "\n",
    "Example (Titanic `sex` with target = `survived`):  \n",
    "- Female → survival rate ≈ 0.74  \n",
    "- Male → survival rate ≈ 0.19  \n",
    "\n",
    "So `sex` becomes:  \n",
    "- Female → 0.74  \n",
    "- Male → 0.19  \n",
    "\n",
    "**Why:**  \n",
    "- Collapses categorical values into **one numeric feature**.  \n",
    "- Very powerful for **high-cardinality features** (like hundreds of ZIP codes).  \n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "tDMHiSLWTEJL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Target Encoding (Mean of survived per category)\n",
    "encoder_target = ce.TargetEncoder(cols=[\"embarked\"])\n",
    "df_target = encoder_target.fit_transform(df[[\"embarked\"]], df[\"survived\"])\n",
    "\n",
    "print(\"After Target Encoding (Embarked):\")\n",
    "print(df_target)\n",
    "# Replace categories with mean of target (survived)."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-aT9iJlezq9",
    "outputId": "9f2a0cc2-be76-411d-98d6-a6cf7ba44c36"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## ✅ Binary Encoding\n",
    "\n",
    "**What it does:**  \n",
    "- Converts category indices into **binary digits** and places them across new columns.  \n",
    "- A balance between Label and One-Hot: fewer columns than OHE, but no fake ordering.  \n",
    "\n",
    "Example (imagine `embarked` has categories C=0, Q=1, S=2):  \n",
    "- 0 → 00  \n",
    "- 1 → 01  \n",
    "- 2 → 10  \n",
    "\n",
    "So we get two binary columns:  \n",
    "\n",
    "| embarked | bin1 | bin2 |\n",
    "|----------|------|------|\n",
    "| C (0)    |  0   |  0   |\n",
    "| Q (1)    |  0   |  1   |\n",
    "| S (2)    |  1   |  0   |\n",
    "\n",
    "**Use case:**  \n",
    "- Good for **moderate or high-cardinality** features where OHE would explode in dimensionality.  \n",
    "- Still keeps categories separated better than plain Label Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    " **Key Takeaway:**  \n",
    "- Choose encoding by **nature of the feature** and **model type**.  \n",
    "- Use Ordinal for true orders, One-Hot for nominal categories in linear models, Target/Binary for high-cardinality, Helmert for statistical interpretability.\n"
   ],
   "metadata": {
    "id": "gHByXPbbTJRi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Binary Encoding\n",
    "encoder = ce.BinaryEncoder(cols=[\"embarked\"])\n",
    "df_binary = encoder.fit_transform(df[[\"embarked\"]])\n",
    "\n",
    "print(\"Original values:\\n\", df[\"embarked\"].unique())\n",
    "print(\"\\nAfter Binary Encoding:\\n\", df_binary)\n",
    "# Binary Encoding (convert categories into binary digits → fewer columns than one-hot)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EP2XQECBcjjK",
    "outputId": "ced1d572-90ef-4d56-9081-475f83fea782"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  Summary Table\n",
    "\n",
    "| Encoding Type   | Best For | Pros | Cons |\n",
    "|-----------------|----------|------|------|\n",
    "| **Ordinal**     | Ordered categories (class, size) | Simple, preserves hierarchy | Wrong if order is artificial |\n",
    "| **Target**      | High-cardinality categorical | Compresses info, powerful | Risk of leakage, must use CV |\n",
    "| **Binary**      | Medium/high-cardinality | Fewer features than OHE | Less interpretable |\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "MV6JqdKoTXvO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic').dropna(subset=['fare'])\n",
    "X = titanic[['pclass','sex','age','sibsp','parch','embarked']]\n",
    "y = titanic['fare']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Columns\n",
    "num_features = ['pclass','age','sibsp','parch']\n",
    "cat_features = ['sex','embarked']\n",
    "\n",
    "def build_pipe(scaler, encoder):\n",
    "    num_pipe = Pipeline([\n",
    "        ('imp', SimpleImputer(strategy='median')),\n",
    "        ('scale', scaler)\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encode', encoder)\n",
    "    ])\n",
    "    pre = ColumnTransformer([\n",
    "        ('num', num_pipe, num_features),\n",
    "        ('cat', cat_pipe, cat_features)\n",
    "    ])\n",
    "    return Pipeline([('pre', pre), ('lr', LinearRegression())])\n",
    "\n",
    "# Pipelines to compare\n",
    "pipelines = {\n",
    "    \"Normalization + OHE\": build_pipe(MinMaxScaler(), OneHotEncoder(drop='first', handle_unknown='ignore')),\n",
    "    \"Standardization + OHE\": build_pipe(StandardScaler(), OneHotEncoder(drop='first', handle_unknown='ignore')),\n",
    "    \"Standardization + Label\": build_pipe(StandardScaler(), OrdinalEncoder()),\n",
    "}\n",
    "\n",
    "# Fit, predict, and compare\n",
    "results = []\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = mean_squared_error(y_test, preds) ** 0.5\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    results.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by=\"RMSE\")\n",
    "print(df_results.to_string(index=False))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZSH8H5n_Vbt",
    "outputId": "0fac42bd-cc85-41b4-b158-66d7c23e083e"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL9eKbXDcx-5"
      },
      "outputs": [],
      "source": [
        "# 1. Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6n-30SFhw1p",
        "outputId": "ce7bb18b-73f0-4c19-ffe6-6f954a6dfd9e"
      },
      "outputs": [],
      "source": [
        "!pip install category_encoders\n",
        "\n",
        "import category_encoders as ce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB9njTo7fcNi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import (MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler, QuantileTransformer,\n",
        "    LabelEncoder, OneHotEncoder, OrdinalEncoder)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-RvgG_GTfd9Q",
        "outputId": "00021002-5e34-4436-a81f-a291d88692f7"
      },
      "outputs": [],
      "source": [
        "# 2. Load Dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Select features & target\n",
        "df = titanic[['class', 'sex', 'age', 'fare', 'embarked', 'survived']].copy()\n",
        "\n",
        "# Introduce some missing values for demo (already present in Titanic)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu1K5D7udYUf"
      },
      "outputs": [],
      "source": [
        "df = df[[\"age\", \"fare\", \"sex\", \"class\", \"embarked\",  \"survived\"]].dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AtiIbShg-IT",
        "outputId": "7e90bab0-86f9-4f10-ff00-b56f53d9f991"
      },
      "outputs": [],
      "source": [
        "# 3. Basic Data Info\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY03gDmmoX4S",
        "outputId": "1da65f32-b42e-4106-d7c7-07f4b6f1a413"
      },
      "outputs": [],
      "source": [
        "print(df.describe(include=\"all\").T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy1ENqfxhQfs"
      },
      "source": [
        "info() shows datatypes (which helps us know which features are categorical/numeric).\n",
        "\n",
        "describe() summarizes statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5GUhVSXXhCtG",
        "outputId": "8c02578f-ba95-4189-bdcc-5ab590c9fb12"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.dropna(inplace=True)  # drop rows with missing values\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfHJ5xpCjofE"
      },
      "source": [
        "# **Scaling (Normalization & Standardization)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tRTU-tZPCcR"
      },
      "source": [
        "#  Feature Scaling: Normalization vs. Standardization\n",
        "\n",
        "Feature scaling is an essential preprocessing step in machine learning.  \n",
        "Many algorithms (Linear Regression, Logistic Regression, SVMs, PCA, K-means, Neural Networks) rely on **distances, dot-products, or gradient descent**. If features are on very different scales, the model may become biased toward larger-magnitude features.\n",
        "\n",
        "---\n",
        "\n",
        "## Normalization (Minâ€“Max Scaling)\n",
        "\n",
        "**Definition:**  \n",
        "Rescales values of a feature into a fixed range, usually [0, 1].  \n",
        "\n",
        "\\[$\n",
        "x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$\n",
        "\\]\n",
        "\n",
        "- Each featureâ€™s smallest value becomes 0, largest becomes 1.  \n",
        "- Preserves the shape of the distribution, but **compresses** the scale.  \n",
        "- Very sensitive to **outliers** (a single extreme value can stretch the scale).  \n",
        "\n",
        "**When to use:**  \n",
        "- Works well when features are bounded (e.g., pixel intensities between 0â€“255).  \n",
        "- Useful in **Neural Networks** where inputs are often normalized.  \n",
        "- Less suitable for features with heavy outliers.\n",
        "\n",
        "---\n",
        "\n",
        "## Standardization (Z-score Scaling)\n",
        "\n",
        "**Definition:**  \n",
        "Centers the feature at mean 0 and scales it to unit variance.  \n",
        "\n",
        "\\[$\n",
        "x' = \\frac{x - \\mu}{\\sigma}$\n",
        "\\]\n",
        "\n",
        "where \\(\\mu\\) is the feature mean and \\(\\sigma\\) is the standard deviation.  \n",
        "\n",
        "- The transformed feature has mean â‰ˆ 0 and standard deviation â‰ˆ 1.  \n",
        "- Not limited to [0, 1]; values can be negative or greater than 1.  \n",
        "- More **robust** than normalization when dealing with outliers.  \n",
        "- Assumes features are roughly Gaussian (bell-shaped) for best results.  \n",
        "\n",
        "**When to use:**  \n",
        "- Default choice for **Linear/Logistic Regression, SVMs, PCA, and K-means**.  \n",
        "- Works well when features have different units (e.g., age in years, fare in dollars).  \n",
        "- Preserves outlier influence without compressing them to 0â€“1.\n",
        "\n",
        "---\n",
        "\n",
        "##  Choosing Between Normalization and Standardization\n",
        "\n",
        "- If your model relies on **distance or dot-product geometry** (linear regression, SVM, PCA, neural networks) â†’ **Standardization** is often better.  \n",
        "- If you need features in a **bounded range** (e.g., image intensities, probability inputs) â†’ **Normalization** works well.  \n",
        "- With **tree-based models** (Decision Trees, Random Forests, XGBoost, CatBoost), scaling usually **doesnâ€™t matter** because splits are not distance-based.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Titanic Example (numeric features only)\n",
        "\n",
        "In the Titanic dataset, consider two features:  \n",
        "- **`age`** (in years, typically 0â€“80 but with some missing values).  \n",
        "- **`fare`** (in dollars, highly skewed with very large outliers).  \n",
        "\n",
        "- With **Normalization**, `fare` outliers (very expensive tickets) will squeeze the majority of passenger fares into a very narrow [0â€“0.1] range.  \n",
        "- With **Standardization**, `fare` will be centered and scaled, but outliers will appear as large positive z-scores, which keeps more useful variation for linear models.  \n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**  \n",
        "- **Normalization:** good when bounded [0,1] values are needed, but sensitive to outliers.  \n",
        "- **Standardization:** good for most ML algorithms, especially linear ones; robust to varying feature units.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "r0La29mxobDI",
        "outputId": "4d5fdafc-148b-4238-d9ff-b451db2e8cbf"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_minmax = df.copy()\n",
        "df_minmax[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
        "\n",
        "print(df_minmax.head())\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
        "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
        "df_minmax[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"MinMax Normalized Fare\", color=\"orange\")\n",
        "plt.show()\n",
        "# Rescales values between 0 and 1. Useful when features have different ranges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "aR4sqJ8dotZ3",
        "outputId": "88801362-f779-423f-d0a5-11bfe1aee06c"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "df_standard = df.copy()\n",
        "df_standard[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
        "\n",
        "print(df_standard.head())\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
        "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
        "df_standard[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"Standardized Fare\", color=\"green\")\n",
        "plt.show()\n",
        "# Transforms to mean=0, std=1. Best for models assuming Gaussian-like features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg8oXdr8Q6yo"
      },
      "source": [
        "##  QuantileTransformer\n",
        "\n",
        "**What it does:**  \n",
        "- Transforms features to follow a **target distribution** by mapping quantiles.  \n",
        "- Two main options in scikit-learn:  \n",
        "  - `output_distribution=\"uniform\"` â†’ transforms features into [0, 1] uniform distribution.  \n",
        "  - `output_distribution=\"normal\"` â†’ transforms features into standard normal (mean=0, var=1).  \n",
        "\n",
        "**How it works:**  \n",
        "- Computes the empirical CDF (cumulative distribution function) of the data.  \n",
        "- Maps each value to its quantile, then to the chosen output distribution.  \n",
        "\n",
        "**Properties:**  \n",
        "- Makes distributions **more Gaussian-like** (if `normal`).  \n",
        "- Very robust to **outliers** (they get compressed into the tails).  \n",
        "- Non-linear transformation â†’ changes relationships between features.  \n",
        "\n",
        "**When to use:**  \n",
        "- Features are highly skewed, long-tailed, or non-Gaussian.  \n",
        "- Useful before algorithms that are sensitive to non-normality (e.g., linear models, Gaussian-based methods).  \n",
        "- Works well when feature scales are very irregular.  \n",
        "\n",
        "---\n",
        "\n",
        "##  StandardScaler vs. QuantileTransformer\n",
        "\n",
        "| Aspect | StandardScaler | QuantileTransformer |\n",
        "|--------|----------------|----------------------|\n",
        "| Effect on Mean/Variance | Centers to mean 0, var 1 | Shapes data into uniform [0,1] or normal |\n",
        "| Handles Outliers | No (keeps them extreme) | Yes (compresses them into tails) |\n",
        "| Distribution Shape | Preserved (still skewed if original is skewed) | Changed (forces uniform or normal) |\n",
        "| Use Case | Roughly Gaussian data, not too skewed | Highly skewed, heavy-tailed, or irregular data |\n",
        "\n",
        "---\n",
        "\n",
        "##  Titanic Example (numeric feature: `fare`)\n",
        "\n",
        "- **StandardScaler:**  \n",
        "  - `fare` will be centered at 0, variance = 1.  \n",
        "  - Extremely high fares remain large positive z-scores.  \n",
        "- **QuantileTransformer (normal):**  \n",
        "  - `fare` distribution becomes closer to Gaussian.  \n",
        "  - Very expensive tickets are pushed into the far right tail but less extreme.  \n",
        "\n",
        "---\n",
        "\n",
        " **Summary:**  \n",
        "- Use **StandardScaler** when features are moderately well-behaved (close to Gaussian).  \n",
        "- Use **QuantileTransformer** when features are **skewed** or have **outliers**, and you want a Gaussian or uniform output distribution.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "pHQ2wqaOcR4h",
        "outputId": "bf77b401-0aea-4417-bf61-83cf331dac4c"
      },
      "outputs": [],
      "source": [
        "scaler = QuantileTransformer(output_distribution=\"uniform\")\n",
        "df_quantile_uniform = df.copy()\n",
        "df_quantile_uniform[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
        "\n",
        "print(df_quantile_uniform.head())\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
        "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
        "df_quantile_uniform[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"Quantile Uniform Fare\", color=\"brown\")\n",
        "plt.show()\n",
        "# Maps values â†’ uniform [0,1] distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "IHQ4AFmxcX_n",
        "outputId": "7e28506e-6a90-4555-f23e-779828c4765d"
      },
      "outputs": [],
      "source": [
        "scaler = QuantileTransformer(output_distribution=\"normal\")\n",
        "df_quantile_normal = df.copy()\n",
        "df_quantile_normal[[\"age\", \"fare\"]] = scaler.fit_transform(df[[\"age\", \"fare\"]])\n",
        "\n",
        "print(df_quantile_normal.head())\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
        "df[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[0], title=\"Original Fare\")\n",
        "df_quantile_normal[\"fare\"].plot(kind=\"hist\", bins=30, ax=axes[1], title=\"Quantile Normal Fare\", color=\"cyan\")\n",
        "plt.show()\n",
        "# Forces values to follow a normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ICg81EjeRv"
      },
      "source": [
        "# **Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAdgBW37SE1J"
      },
      "source": [
        "#  Why Do We Need Encoding?\n",
        "\n",
        "Machine learning models usually work with **numerical features**.  \n",
        "But in many datasets (like Titanic), we have **categorical features** such as:\n",
        "\n",
        "- `sex` â†’ {male, female}  \n",
        "- `embarked` â†’ {C, Q, S}  \n",
        "- `class` â†’ {First, Second, Third}  \n",
        "\n",
        "These values are **labels**, not numbers.  \n",
        "If we feed them directly into most ML algorithms, they will not understand the categories.  \n",
        "\n",
        " **Encoding** is the process of converting categorical variables into numeric form so that models can use them.\n",
        "\n",
        "---\n",
        "\n",
        "##  Label Encoding\n",
        "\n",
        "**How it works:**  \n",
        "- Assigns each category a unique integer.  \n",
        "\n",
        "Example:  \n",
        "- `sex`: {male â†’ 0, female â†’ 1}  \n",
        "- `embarked`: {C â†’ 0, Q â†’ 1, S â†’ 2}  \n",
        "\n",
        "**Advantages:**  \n",
        "- Simple and compact.  \n",
        "- Works well for **tree-based models** (Decision Trees, Random Forest, XGBoost), since they split based on thresholds, not distances.  \n",
        "\n",
        "**Disadvantages:**  \n",
        "- Imposes a **fake order** (0 < 1 < 2), which can mislead models like Linear Regression, Logistic Regression, or SVM (they may think `S` > `Q` > `C`).  \n",
        "\n",
        "---\n",
        "\n",
        "##  One-Hot Encoding (OHE)\n",
        "\n",
        "**How it works:**  \n",
        "- Creates a new **binary column for each category**.  \n",
        "- Value is `1` if the row belongs to that category, else `0`.  \n",
        "\n",
        "Example: `embarked` â†’ {C, Q, S}  \n",
        "\n",
        "| embarked | embarked_C | embarked_Q | embarked_S |\n",
        "|----------|------------|------------|------------|\n",
        "| C        | 1          | 0          | 0          |\n",
        "| S        | 0          | 0          | 1          |\n",
        "| Q        | 0          | 1          | 0          |\n",
        "\n",
        "**Advantages:**  \n",
        "- Avoids false ordinal relationships.  \n",
        "- Safe default for **linear models, logistic regression, SVMs, and neural networks**.  \n",
        "\n",
        "**Disadvantages:**  \n",
        "- Increases the number of features (can be large if categories are many).  \n",
        "- Can lead to **sparse matrices** with high-cardinality categorical variables.  \n",
        "\n",
        "---\n",
        "\n",
        "## When to Use Which?\n",
        "\n",
        "- **Tree-based models (Decision Trees, Random Forest, Gradient Boosting):**  \n",
        "  â†’ Label Encoding is fine (trees handle categories by splitting, not by distances).  \n",
        "\n",
        "- **Linear models, Logistic Regression, SVMs, Neural Networks:**  \n",
        "  â†’ One-Hot Encoding is preferred (no fake ordering).  \n",
        "\n",
        "- **High cardinality (hundreds/thousands of categories):**  \n",
        "  â†’ Consider advanced techniques like **Target Encoding, Frequency Encoding, or Hashing**.  \n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**  \n",
        "- **Label Encoding** â†’ compact, but risky for linear models.  \n",
        "- **One-Hot Encoding** â†’ more features, but safer for most models.  \n",
        "- Always choose encoding based on **model type** and **data characteristics**.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd5IpA0Nc39Y",
        "outputId": "add16f7a-bd09-4b56-974c-bc3580ec7c6f"
      },
      "outputs": [],
      "source": [
        "categorical_features = [\"sex\", \"embarked\",\"class\"]\n",
        "\n",
        "# 1. Label Encoding\n",
        "le = LabelEncoder()\n",
        "df_label = df.copy()\n",
        "for col in categorical_features:\n",
        "    df_label[col] = le.fit_transform(df[col])\n",
        "print(\"Label Encoding:\\n\", df_label, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMumJlIGc_nb",
        "outputId": "11749cc8-ba9b-44e5-e3df-90fed95dc61f"
      },
      "outputs": [],
      "source": [
        "# 2. One-Hot Encoding\n",
        "df_onehot = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
        "print(\"One-Hot Encoding:\\n\", df_onehot.head(), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dflq0xVSSuXI"
      },
      "source": [
        "# ðŸŽ›ï¸ Beyond One-Hot and Label Encoding: Advanced Encodings\n",
        "\n",
        "\n",
        "##  Ordinal Encoding\n",
        "\n",
        "**What it does:**  \n",
        "- Maps categories to integers **preserving their natural order**.  \n",
        "\n",
        "Example (Titanic `class`):  \n",
        "- First â†’ 3  \n",
        "- Second â†’ 2  \n",
        "- Third â†’ 1  \n",
        "\n",
        "**Why:**  \n",
        "- `class` is truly ordered (First > Second > Third).  \n",
        "- Unlike generic Label Encoding, the ordering here makes sense.  \n",
        "\n",
        "**Use case:**  \n",
        "- Works well for features with meaningful hierarchy (education level, size: small/medium/large, etc.).  \n",
        "- Safe for linear models if the relationship is monotonic.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V629_goxeyRK",
        "outputId": "75bbf38f-f3bc-4d75-f78c-85a5c13b8d8b"
      },
      "outputs": [],
      "source": [
        "# 3. Ordinal Encoding (for ordered categories like 'class')\n",
        "ord_enc = OrdinalEncoder(categories=[[\"Third\", \"Second\", \"First\"]])\n",
        "df_ord = df.copy()\n",
        "df_ord[\"class\"] = ord_enc.fit_transform(df[[\"class\"]])\n",
        "print(\"Ordinal Encoding:\\n\", df_ord[[\"class\"]], \"\\n\")\n",
        "# Ordinal Encoding (for ordered categories like class = [Third < Second < First])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDMHiSLWTEJL"
      },
      "source": [
        "## âœ… Target Encoding\n",
        "\n",
        "**What it does:**  \n",
        "- Replaces each category with a **statistic of the target variable** (commonly the mean).  \n",
        "\n",
        "Example (Titanic `sex` with target = `survived`):  \n",
        "- Female â†’ survival rate â‰ˆ 0.74  \n",
        "- Male â†’ survival rate â‰ˆ 0.19  \n",
        "\n",
        "So `sex` becomes:  \n",
        "- Female â†’ 0.74  \n",
        "- Male â†’ 0.19  \n",
        "\n",
        "**Why:**  \n",
        "- Collapses categorical values into **one numeric feature**.  \n",
        "- Very powerful for **high-cardinality features** (like hundreds of ZIP codes).  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-aT9iJlezq9",
        "outputId": "9f2a0cc2-be76-411d-98d6-a6cf7ba44c36"
      },
      "outputs": [],
      "source": [
        "# 4. Target Encoding (Mean of survived per category)\n",
        "encoder_target = ce.TargetEncoder(cols=[\"embarked\"])\n",
        "df_target = encoder_target.fit_transform(df[[\"embarked\"]], df[\"survived\"])\n",
        "\n",
        "print(\"After Target Encoding (Embarked):\")\n",
        "print(df_target)\n",
        "# Replace categories with mean of target (survived)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHByXPbbTJRi"
      },
      "source": [
        "\n",
        "## âœ… Binary Encoding\n",
        "\n",
        "**What it does:**  \n",
        "- Converts category indices into **binary digits** and places them across new columns.  \n",
        "- A balance between Label and One-Hot: fewer columns than OHE, but no fake ordering.  \n",
        "\n",
        "Example (imagine `embarked` has categories C=0, Q=1, S=2):  \n",
        "- 0 â†’ 00  \n",
        "- 1 â†’ 01  \n",
        "- 2 â†’ 10  \n",
        "\n",
        "So we get two binary columns:  \n",
        "\n",
        "| embarked | bin1 | bin2 |\n",
        "|----------|------|------|\n",
        "| C (0)    |  0   |  0   |\n",
        "| Q (1)    |  0   |  1   |\n",
        "| S (2)    |  1   |  0   |\n",
        "\n",
        "**Use case:**  \n",
        "- Good for **moderate or high-cardinality** features where OHE would explode in dimensionality.  \n",
        "- Still keeps categories separated better than plain Label Encoding.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " **Key Takeaway:**  \n",
        "- Choose encoding by **nature of the feature** and **model type**.  \n",
        "- Use Ordinal for true orders, One-Hot for nominal categories in linear models, Target/Binary for high-cardinality, Helmert for statistical interpretability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP2XQECBcjjK",
        "outputId": "ced1d572-90ef-4d56-9081-475f83fea782"
      },
      "outputs": [],
      "source": [
        "# 5. Binary Encoding\n",
        "encoder = ce.BinaryEncoder(cols=[\"embarked\"])\n",
        "df_binary = encoder.fit_transform(df[[\"embarked\"]])\n",
        "\n",
        "print(\"Original values:\\n\", df[\"embarked\"].unique())\n",
        "print(\"\\nAfter Binary Encoding:\\n\", df_binary)\n",
        "# Binary Encoding (convert categories into binary digits â†’ fewer columns than one-hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV6JqdKoTXvO"
      },
      "source": [
        "\n",
        "##  Summary Table\n",
        "\n",
        "| Encoding Type   | Best For | Pros | Cons |\n",
        "|-----------------|----------|------|------|\n",
        "| **Ordinal**     | Ordered categories (class, size) | Simple, preserves hierarchy | Wrong if order is artificial |\n",
        "| **Target**      | High-cardinality categorical | Compresses info, powerful | Risk of leakage, must use CV |\n",
        "| **Binary**      | Medium/high-cardinality | Fewer features than OHE | Less interpretable |\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZSH8H5n_Vbt",
        "outputId": "0fac42bd-cc85-41b4-b158-66d7c23e083e"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic').dropna(subset=['fare'])\n",
        "X = titanic[['pclass','sex','age','sibsp','parch','embarked']]\n",
        "y = titanic['fare']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Columns\n",
        "num_features = ['pclass','age','sibsp','parch']\n",
        "cat_features = ['sex','embarked']\n",
        "\n",
        "def build_pipe(scaler, encoder):\n",
        "    num_pipe = Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', scaler)\n",
        "    ])\n",
        "    cat_pipe = Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encode', encoder)\n",
        "    ])\n",
        "    pre = ColumnTransformer([\n",
        "        ('num', num_pipe, num_features),\n",
        "        ('cat', cat_pipe, cat_features)\n",
        "    ])\n",
        "    return Pipeline([('pre', pre), ('lr', LinearRegression())])\n",
        "\n",
        "# Pipelines to compare\n",
        "pipelines = {\n",
        "    \"Normalization + OHE\": build_pipe(MinMaxScaler(), OneHotEncoder(drop='first', handle_unknown='ignore')),\n",
        "    \"Standardization + OHE\": build_pipe(StandardScaler(), OneHotEncoder(drop='first', handle_unknown='ignore')),\n",
        "    \"Standardization + Label\": build_pipe(StandardScaler(), OrdinalEncoder()),\n",
        "}\n",
        "\n",
        "# Fit, predict, and compare\n",
        "results = []\n",
        "for name, pipe in pipelines.items():\n",
        "    pipe.fit(X_train, y_train)\n",
        "    preds = pipe.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    rmse = mean_squared_error(y_test, preds) ** 0.5\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    results.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
        "\n",
        "df_results = pd.DataFrame(results).sort_values(by=\"RMSE\")\n",
        "print(df_results.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

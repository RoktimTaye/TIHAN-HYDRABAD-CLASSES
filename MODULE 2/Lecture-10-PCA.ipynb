{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovph_prXBLuq"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction & Transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTm9rSQfBX1P",
        "outputId": "ff8f4dc7-e413-418d-93e6-f69ce72404ef"
      },
      "outputs": [],
      "source": [
        "# Part 1: Creating Interaction Features (Titanic dataset)\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "print(\"Shape before cleaning:\", titanic.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZoV25WQBanU",
        "outputId": "5c1e103a-cbdc-4eb1-9684-e116dca2dd17"
      },
      "outputs": [],
      "source": [
        "# Select relevant columns\n",
        "df = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()\n",
        "# .dropna() → removes rows with missing values so we don’t run into errors\n",
        "print(\"Shape after cleaning:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CUJO-dKyBuUa",
        "outputId": "bab10e66-e0bc-473f-9dd9-8fbaf55c9b3f"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzo9PIi5Bv4G",
        "outputId": "13fe21ac-ae57-47d5-c585-086cd44f9ffc"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKLNgqjDcya"
      },
      "source": [
        "survived → target variable (0 = died, 1 = survived).\n",
        "\n",
        "pclass → passenger class (1st, 2nd, 3rd).\n",
        "\n",
        "sex, age, sibsp (siblings/spouses), parch (parents/children), fare, embarked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub-2XHJaBcVV"
      },
      "outputs": [],
      "source": [
        "# Create interaction features\n",
        "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
        "# Creates a new feature: total number of people traveling with passenger (including self).\n",
        "# Example: If sibsp=1, parch=2 → family_size = 1 + 2 + 1 = 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3GyBFLGDjxF"
      },
      "outputs": [],
      "source": [
        "df['age_fare_interaction'] = df['age'] * df['fare']\n",
        "# Creates another interaction feature by multiplying age × fare.\n",
        "# This could capture relationships like: “Do older, richer people (high fare) survive differently?”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwsJDTkNDlYm"
      },
      "outputs": [],
      "source": [
        "df['is_child'] = (df['age'] < 12).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AyBdGSyD4mO"
      },
      "source": [
        "Creates a binary feature:\n",
        "\n",
        "1 if passenger is a child (age < 12).\n",
        "\n",
        "0 otherwise.\n",
        "\n",
        ".astype(int) → converts boolean (True/False) into numeric (1/0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_WxZup6BfcD"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "print(\"\\nSample with new features:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJe5hUH7EALZ"
      },
      "source": [
        "Converts categorical columns (sex, embarked) into numeric using one-hot encoding.\n",
        "\n",
        "Example: sex → replaced with sex_male (1 if male, 0 if female).\n",
        "\n",
        "drop_first=True → avoids dummy variable trap (removes redundant column)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYB7aCz7BzqJ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf9sELGGEx8a"
      },
      "source": [
        "What is PCA?\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used in Machine Learning. It transforms a dataset with many features into a smaller set of new features (called principal components) while keeping as much information (variance) as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k179L0XE5Du"
      },
      "source": [
        "PCA helps in:\n",
        "\n",
        "Reducing noise.\n",
        "\n",
        "Avoiding overfitting.\n",
        "\n",
        "Visualizing high-dimensional data in 2D or 3D."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaN5cLVDZZDd"
      },
      "source": [
        "**Example of PCA on IRIS dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxSFrUmXAQNU"
      },
      "outputs": [],
      "source": [
        "# Part 2: PCA on Iris Dataset\n",
        "\n",
        "from sklearn.datasets import load_iris\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbsW_8V9B_tE",
        "outputId": "36bf28cf-2ce0-491c-fed3-83e269282717"
      },
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "print(X[:10])\n",
        "print(y[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1vgBeFcEIGt"
      },
      "source": [
        "Loads the Iris dataset (classic ML dataset: flower measurements).\n",
        "\n",
        "X → input features (4 columns: sepal length, sepal width, petal length, petal width).\n",
        "\n",
        "y → target (0,1,2 = species)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikjavtd4CLge"
      },
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hgn9L9UENl9"
      },
      "source": [
        "PCA requires data to be standardized (mean = 0, variance = 1).\n",
        "\n",
        "StandardScaler does this:\n",
        "\n",
        "Subtracts mean, divides by standard deviation.\n",
        "\n",
        "fit_transform(X) → fits scaler on data and applies transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll05UuJFCCc-",
        "outputId": "c9a6403d-f353-413a-ab93-1c66ee0bcf66"
      },
      "outputs": [],
      "source": [
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"\\nExplained variance ratio:\", pca.explained_variance_ratio_)\n",
        "# Shows how much information (variance) is preserved in PC1 & PC2.\n",
        "# Example: [0.73, 0.23] means PC1 explains 73% of variance, PC2 explains 23%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHPU85wSETT6"
      },
      "source": [
        "Creates a PCA object with 2 components (reduce 4D → 2D).\n",
        "\n",
        "fit_transform → learns PCA transformation and applies it.\n",
        "\n",
        "X_pca → new dataset with 2 principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "rQqGhvfqCEO8",
        "outputId": "5727d7df-62e8-4a3b-f097-11774b29af7f"
      },
      "outputs": [],
      "source": [
        "# Plot PCA\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis', edgecolor='k', s=80)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA on Iris Dataset\")\n",
        "plt.colorbar(label=\"Species\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0Br-QzHZkF5"
      },
      "source": [
        "**Example of PCA on Titanic Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ6SOSPfbclg"
      },
      "outputs": [],
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1) Load data\n",
        "df = sns.load_dataset(\"titanic\").copy()\n",
        "\n",
        "# We'll use only numeric columns to keep it simple\n",
        "num_cols = [\"age\", \"fare\", \"pclass\", \"sibsp\", \"parch\"]\n",
        "\n",
        "df = df.dropna(subset=[\"survived\"]).copy()\n",
        "y = df[\"survived\"].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cCzj_O20y-S"
      },
      "outputs": [],
      "source": [
        "#  Simple imputation for numerics (median)\n",
        "for c in num_cols:\n",
        "    df[c] = df[c].fillna(df[c].median())\n",
        "\n",
        "X = df[num_cols].astype(float)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsandv7d0_1z",
        "outputId": "a4ea854d-2eac-4ae7-ddf5-f48793b246db"
      },
      "outputs": [],
      "source": [
        "# PCA to 2 components\n",
        "pca = PCA(n_components=2, svd_solver=\"full\")\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "evr = pca.explained_variance_ratio_\n",
        "print(\"Explained variance ratio (PC1, PC2):\", evr, \" | cumulative:\", evr.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "ZIv1_ukRm1Ga",
        "outputId": "4690d683-82c6-46aa-cf4d-b5f05e420b41"
      },
      "outputs": [],
      "source": [
        "# 5) BEFORE: Raw-space visualization (pick two original features)\n",
        "plt.figure()\n",
        "mask0 = (y == 0).values\n",
        "mask1 = (y == 1).values\n",
        "plt.scatter(X.loc[mask0, \"age\"], X.loc[mask0, \"fare\"], alpha=0.6, label=\"Died (y=0)\")\n",
        "plt.scatter(X.loc[mask1, \"age\"], X.loc[mask1, \"fare\"], alpha=0.6, label=\"Survived (y=1)\")\n",
        "plt.xlabel(\"age\"); plt.ylabel(\"fare\"); plt.title(\"BEFORE: Raw features (age vs fare)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6) AFTER: PCA-space visualization (PC1 vs PC2)\n",
        "plt.figure()\n",
        "plt.scatter(X_pca[mask0, 0], X_pca[mask0, 1], alpha=0.6, label=\"Died (y=0)\")\n",
        "plt.scatter(X_pca[mask1, 0], X_pca[mask1, 1], alpha=0.6, label=\"Survived (y=1)\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.title(\"AFTER: PCA projection (PC1 vs PC2)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnN2A2nIZzFS"
      },
      "source": [
        "**Comparison of ANOVA selecting two high variance features and PCA selecting two principal components**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "fs3lA_Ax3HLa",
        "outputId": "09d78600-cf9d-4601-dd1f-d433b6966b86"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load & prep\n",
        "df = sns.load_dataset(\"titanic\").dropna(subset=[\"survived\"]).copy()\n",
        "y = df[\"survived\"].astype(int)\n",
        "num = [\"age\",\"fare\",\"pclass\",\"sibsp\",\"parch\"]\n",
        "for c in num: df[c] = df[c].fillna(df[c].median())\n",
        "X = df[num].astype(float)\n",
        "\n",
        "# Pick best two raw features (univariate) using ANOVA\n",
        "fvals, _ = f_classif(X, y)\n",
        "best2 = np.argsort(fvals)[-2:]\n",
        "f1, f2 = X.columns[best2]\n",
        "\n",
        "# Plot baseline: best two raw features\n",
        "plt.figure();\n",
        "plt.scatter(X[f1][y==0], X[f2][y==0], alpha=0.6, label=\"Died\")\n",
        "plt.scatter(X[f1][y==1], X[f2][y==1], alpha=0.6, label=\"Survived\")\n",
        "plt.xlabel(f1); plt.ylabel(f2); plt.title(\"Baseline: best two raw features\"); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# PCA on the same inputs\n",
        "X_std = StandardScaler().fit_transform(X)\n",
        "X_pca = PCA(n_components=2, svd_solver=\"full\").fit_transform(X_std)\n",
        "\n",
        "# Plot PCA: PC1 vs PC2\n",
        "plt.figure();\n",
        "plt.scatter(X_pca[y==0,0], X_pca[y==0,1], alpha=0.6, label=\"Died\")\n",
        "plt.scatter(X_pca[y==1,0], X_pca[y==1,1], alpha=0.6, label=\"Survived\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.title(\"PCA: projection of all numeric features\"); plt.legend(); plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wtE-oGZ-Uc"
      },
      "source": [
        "How PCA impacts the accuracy of the models: Applying logistic regression  (i) on seven features titanic dataset and (ii) two features obtained through PCA  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4P1q_G05bHD",
        "outputId": "4e016520-23f5-44f6-e71a-2d7ddf5c8542"
      },
      "outputs": [],
      "source": [
        "# Titanic: \"All features\" vs \"PCA → 2 features\" (no pipelines)\n",
        "import seaborn as sns, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# 1) Load & target\n",
        "df = sns.load_dataset(\"titanic\").copy()\n",
        "df = df.dropna(subset=[\"survived\"])\n",
        "y = df[\"survived\"].astype(int)\n",
        "\n",
        "# 2) Choose a modest feature set (numeric + a couple categoricals)\n",
        "num_cols = [\"age\", \"fare\", \"pclass\", \"sibsp\", \"parch\"]\n",
        "cat_cols = [\"sex\", \"embarked\"]   # simple, low-cardinality\n",
        "\n",
        "# Impute: median for numeric, mode for categoricals\n",
        "for c in num_cols:\n",
        "    df[c] = df[c].fillna(df[c].median())\n",
        "for c in cat_cols:\n",
        "    df[c] = df[c].fillna(df[c].mode().iloc[0])\n",
        "\n",
        "# One-hot encode categoricals (no label info leakage)\n",
        "X = pd.get_dummies(df[num_cols + cat_cols], drop_first=True)\n",
        "\n",
        "# Train/test split (stratified)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Scale (fit on train, apply to test)\n",
        "scaler = StandardScaler()\n",
        "Xtr_sc = scaler.fit_transform(X_tr)\n",
        "Xte_sc = scaler.transform(X_te)\n",
        "\n",
        "# 4A) Baseline: Logistic Regression on ALL features\n",
        "clf_all = LogisticRegression(max_iter=1000)\n",
        "clf_all.fit(Xtr_sc, y_tr)\n",
        "proba_all = clf_all.predict_proba(Xte_sc)[:, 1]\n",
        "pred_all = (proba_all >= 0.5).astype(int)\n",
        "print(\"ALL features — Acc:\", accuracy_score(y_te, pred_all),\n",
        "      \"AUC:\", roc_auc_score(y_te, proba_all), \"| dims:\", Xtr_sc.shape[1])\n",
        "\n",
        "# 4B) PCA → 2 components, then Logistic Regression\n",
        "pca = PCA(n_components=2, svd_solver=\"full\")\n",
        "Xtr_pca = pca.fit_transform(Xtr_sc)\n",
        "Xte_pca = pca.transform(Xte_sc)\n",
        "print(\"PCA EVR (PC1, PC2):\", pca.explained_variance_ratio_,\n",
        "      \"| cumulative:\", pca.explained_variance_ratio_.sum())\n",
        "\n",
        "clf_pca2 = LogisticRegression(max_iter=1000)\n",
        "clf_pca2.fit(Xtr_pca, y_tr)\n",
        "proba_pca2 = clf_pca2.predict_proba(Xte_pca)[:, 1]\n",
        "pred_pca2 = (proba_pca2 >= 0.5).astype(int)\n",
        "print(\"PCA→2 comps — Acc:\", accuracy_score(y_te, pred_pca2),\n",
        "      \"AUC:\", roc_auc_score(y_te, proba_pca2), \"| dims:\", Xtr_pca.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6DGdGNjFC5D"
      },
      "source": [
        "Advantages of PCA\n",
        "\n",
        "Reduces dataset size while keeping important info.\n",
        "\n",
        "Improves training speed of ML models.\n",
        "\n",
        "Helps visualization.\n",
        "\n",
        "Removes multicollinearity (features that are highly correlated).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvhY9n6KFFjZ"
      },
      "source": [
        "Limitations\n",
        "\n",
        "PCA is unsupervised (doesn’t consider target labels).\n",
        "\n",
        "Sometimes difficult to interpret PCs (they are combinations of original features).\n",
        "\n",
        "Works best when linear relationships exist."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

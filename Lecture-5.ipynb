{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noSzl8sv2Z53"
   },
   "source": [
    "# **Setup and Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C4twbtJ_1D0A"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aw00Hjp51IN_"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "zpNNbMNa1Jyp",
    "outputId": "52a19681-5675-4abe-e1ec-078aab6d2128"
   },
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select features & target\n",
    "df = titanic[['pclass', 'sex', 'age', 'fare', 'embarked', 'survived']].copy()\n",
    "\n",
    "# Introduce some missing values for demo (already present in Titanic)\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Z0Gvec2HaU"
   },
   "source": [
    "# **Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP7jJ7SJ2G44"
   },
   "source": [
    "Before we train any model, it's crucial to split our data. We need a separate set of data that the model has never seen before to evaluate its performance fairly. This prevents \"data leakage\" and gives us a more realistic measure of how our model will perform on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xi5EkHQV1Mwx",
    "outputId": "90a8ad85-f781-4ee3-faa8-4f7483f01575"
   },
   "source": [
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)\n",
    "print(\"Training output size:\", y_train.shape)\n",
    "print(\"Testing output size:\", y_test.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Tns38-f2lqU"
   },
   "source": [
    "test_size=0.2 means 20% of the data will be used for testing, and 80% for training.\n",
    "\n",
    "random_state=42 is a common practice that ensures the same split is generated every time the code is run. This is important for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CWILkNw6ASL"
   },
   "source": [
    "# **Preprocessing for Columns(handling missing values, scaling, encoding)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdQQHs8gAhf7"
   },
   "source": [
    "SimpleImputer(strategy='mean')\n",
    "\n",
    "Looks for missing values (NaN) in numeric columns\n",
    "\n",
    "Replaces them with the column’s mean value\n",
    "\n",
    "StandardScaler()\n",
    "\n",
    "Subtracts the mean from each value\n",
    "\n",
    "Divides by the standard deviation\n",
    "\n",
    "Result: all numeric features have mean = 0 and std = 1\n",
    "\n",
    "Why? Many models (Logistic Regression, SVM, etc.) perform better when numeric features are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6s2NPRfW5_jt"
   },
   "source": [
    "# Separate column types\n",
    "numeric_features = ['age', 'fare', 'pclass']\n",
    "categorical_features = ['sex', 'embarked']\n",
    "\n",
    "# Numeric pipeline: handle missing values → scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfYlSq7rArwS"
   },
   "source": [
    "SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "Fills missing values with the most common value in that column (mode)\n",
    "\n",
    "OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "Converts categories into binary columns (e.g., \"male\" → [1,0], \"female\" → [0,1])\n",
    "\n",
    "handle_unknown='ignore' prevents errors if the test set has a category not seen in training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6TUsbvF7BBdV"
   },
   "source": [
    "# Categorical pipeline: handle missing values → encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mcpD-TymBERe"
   },
   "source": [
    "# Combine into column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "# Applies numeric_transformer only to columns in numeric_features\n",
    "# Applies categorical_transformer only to columns in categorical_features\n",
    "# Keeps everything together so when you fit the model, preprocessing happens automatically."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer on Logistic Regression:\n",
    "\n",
    "## Logistic Regression (simple intro)\n",
    "\n",
    "**Goal:** binary classification (Titanic: $0 =$ died, $1 =$ survived).\n",
    "\n",
    "1. **Linear score** from features $x_1,\\ldots,x_d$:\n",
    "$$\n",
    "\\text{score} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d\n",
    "$$\n",
    "\n",
    "2. **Sigmoid to probability**:\n",
    "$$\n",
    "\\hat{p} = \\Pr(y=1 \\mid \\mathbf{x}) = \\sigma(\\text{score}) = \\frac{1}{1 + e^{-\\text{score}}}, \\qquad \\hat{p}\\in(0,1)\n",
    "$$\n",
    "\n",
    "3. **Turn probability into class label** (threshold $\\tau$, default $0.5$):\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\hat{p} \\ge \\tau,\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "4. **Learning** = choose weights $\\{w_i\\}$ by minimizing **log loss**:\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\Big[ y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i) \\Big].\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E434Y2O57itr"
   },
   "source": [
    "model = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=500))\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5gS_hA8BaeH"
   },
   "source": [
    "Because of preprocessing the model never sees raw data. Every time you call .fit() or .predict():\n",
    "\n",
    "Missing values get filled\n",
    "\n",
    "Scaling happens\n",
    "\n",
    "Categories get encoded\n",
    "\n",
    "The cleaned data is passed to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QjIPfPt8r81"
   },
   "source": [
    "# **Fit/Predict**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "CKXrYQ4f4M7O",
    "outputId": "ce4e5715-184a-4331-9b4e-e301afceb657"
   },
   "source": [
    "# Fit model on training data\n",
    "model.fit(X_train, y_train)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JkLn8KKH7vhd"
   },
   "source": [
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cQOhc3p8u_t"
   },
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro6h8IkR7v6k",
    "outputId": "51732ab6-b6d5-4302-ae82-2cec104c2a32"
   },
   "source": [
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix:\n",
    "\n",
    "TP: predicted survived, actually survived\n",
    "\n",
    "TN: predicted died, actually died\n",
    "\n",
    "FP: predicted survived, actually died (false alarm)\n",
    "\n",
    "FN: predicted died, actually survived (missed survivor)\n",
    "\n",
    "                        Predicted: died (0)\t        Predicted: survived (1)\n",
    "Actual: died (0)\t    TN (true negatives)\t        FP (false positives)\n",
    "Actual: survived (1)\tFN (false negatives)\t    TP (true positives)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lerwxbuP7zhs",
    "outputId": "76675b0a-ff6a-415d-df94-0d14a24a7f88"
   },
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class (here: 0 and 1) you get four columns:\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "“Of the passengers I predicted as this class, how many were correct?”\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "“Of the passengers that truly are this class, how many did I find?”\n",
    "\n",
    "f1-score = 2 · (precision · recall) / (precision + recall)\n",
    "Harmonic mean—high only if both precision and recall are high.\n",
    "\n",
    "It ignores true negatives, so it focuses on how well you find and correctly label the positive examples of that class.\n",
    "\n",
    "When to use:\n",
    "\n",
    "Imbalanced data (like Titanic, where fewer people survived).\n",
    "\n",
    "When both false positives and false negatives are costly.\n",
    "\n",
    "support = number of true samples of that class in y_test.\n",
    "\n",
    "You will also see:\n",
    "\n",
    "accuracy: overall fraction correct predictions across all the test samples.\n",
    "\n",
    "macro avg: simple (unweighted) average of precision/recall/F1 across classes (treats classes equally, regardless of size).\n",
    "\n",
    "weighted avg: averages weighted by support (dominated by the larger class)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrdCMBJp700W",
    "outputId": "d3169d0e-f24d-4acc-fca2-ea07d4b6a7aa"
   },
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret for Titanic datset:\n",
    "Class 0 (died) usually has higher support (more passengers died than survived).\n",
    "→ Weighted averages are often closer to class 0’s scores.\n",
    "\n",
    "If your goal is to catch survivors (class 1), focus on recall for class 1 (don’t miss survivors), and check its F1.\n",
    "\n",
    "If false alarms are costly, focus on precision for class 1.\n",
    "\n",
    "\n",
    "Imbalanced data: accuracy can look good even with poor minority-class performance. Use macro/F1 and per-class scores."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "aQSiYYK572Dc",
    "outputId": "cb0b4ba8-026c-420f-dc04-203b6dcc495a"
   },
   "source": [
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
